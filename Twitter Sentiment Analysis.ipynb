{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Machine learning mini-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How is your prediction task defined? Classification or regression, and what is the meaning of the output variable?\n",
    "The prediction task is a classification where we read the twitter messages and analyse the sentiments; That is, classify the tweets as positive, negative or neutral.\n",
    "The output variable Y is the labelling (postive/negative/neutral).\n",
    "\n",
    "2. Where did you get the data? (Not necessary if you're working on the Twitter task.)\n",
    "Twitter task\n",
    "\n",
    "3. How do you represent your data as features?\n",
    "Initially we used a bag of words representation for our features. In order to add clustering, we made it into a dictionary of clustername and the count which tells how often we encountered a word from that particular cluster.\n",
    "\n",
    "4. Did you process the features in any way?\n",
    "Yes, we used the feature_extraction(DictVectorizer) and feature_selection(SelectKBest) for processing the features.  \n",
    "\n",
    "5. Did you bring in any additional sources of data?\n",
    "Yes, we used the word clusters that were computed by Olutobi Owoputi and colleagues at Carnegie Mellon University, given in the assignment page.\n",
    "\n",
    "6. How did you select which learning algorithms to use?\n",
    "We used the different algorithms for classification and this one gave us the best results.\n",
    "\n",
    "7. Did you try to tune the hyperparameters of the learning algorithm, and in that case how?\n",
    "Yes, we used the Random Search procedure to tune the hyperparameters C and penalty of LogisticReqression.\n",
    "\n",
    "8. How do you evaluate the quality of your system?\n",
    "We used labeled test data to evaluate the trained classifier and it gives an accuracy score of 0.62.\n",
    "\n",
    "9. How well does your system compare to a stupid baseline?\n",
    "DummyClassifier gives an accuracy score of 0.377233620119 whereas our trained classifer gives 0.62.\n",
    "\n",
    "10. Can you say anything about the errors that the system makes? For a classification task, you may consider a confusion matrix.\n",
    "The confusion matrix for the labels [\"positive\", \"negative\", \"neutral\"] is:\n",
    "[[200  84 242]\n",
    " [104  41 163]\n",
    " [247 104 326]]\n",
    "\n",
    "Our classifier was mixing up Positive and Neutral (242 and 247) most often. But Positive and Negative (84 & 104) was rarely mixed up. Negative was very rarely guessed correctly. But then, Neutral and Positive were good.\n",
    "\n",
    "11. Is it possible to say something about which features the model considers important? (Whether this is possible depends on the type of classifier you are using.)\n",
    "The classifier chooses the ones with the highest count values for the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.618795499669\n",
      "0.374586366645\n",
      "[[207  74 245]\n",
      " [117  44 147]\n",
      " [266  96 315]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon\n",
    "from collections import defaultdict\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "tokenize_re = re.compile(r'''\n",
    "                         \\d+[:\\.]\\d+\n",
    "                         |(https?://)?(\\w+\\.)(\\w{2,})+([\\w/]+)\n",
    "                         |[@\\#]?\\w+(?:[-']\\w+)*\n",
    "                         |[^a-zA-Z0-9 ]+''',\n",
    "                         re.VERBOSE)\n",
    "\n",
    "def tokenize(text):\n",
    "    return [ m.group() for m in tokenize_re.finditer(text) ]\n",
    "\n",
    "def print_token(filename):\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            print(tokenize(l))\n",
    "            break\n",
    "            \n",
    "            \n",
    "clusters = defaultdict(str)\n",
    "\n",
    "\n",
    "def create_clusters(filename):\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "            for l in f:\n",
    "                cols = l.strip('\\n').split('\\t')\n",
    "                clustername = cols[0]\n",
    "                token = cols[1]\n",
    "                # check if the token is already in a cluster; if not -> add it\n",
    "                if token not in clusters:\n",
    "                    clusters[token] = clustername\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# Funct        \n",
    "def read_word_clusters(filename):\n",
    "    tokens = []\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            l = l.strip('\\n')\n",
    "            tokens = tokenize(l)\n",
    "            tweet_clusters = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            # Read from tokens[2] and compare every word with clusters \n",
    "            # If the word is in clusters, get the clustername else ignore it\n",
    "            # If the clustername & count is present in tweet_clusters, update the count, else\n",
    "            # insert the entry into tweet_clusters\n",
    "            for t in tokens[2:]:\n",
    "                if t in clusters:\n",
    "                    c = clusters[t]\n",
    "                    if c in tweet_clusters:\n",
    "                        count = tweet_clusters[c]\n",
    "                        tweet_clusters.update({c:count+1})\n",
    "                    else:\n",
    "                        tweet_clusters[c] = 1\n",
    "            X.append(tweet_clusters)\n",
    "            Y.append(tokens[0])\n",
    "    return X, Y\n",
    "        \n",
    "# Initially we represented the features as a bag of words and used CountVectorizer    \n",
    "def read_feature_descriptions(filename):\n",
    "    tokens = []\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename) as f:\n",
    "        for l in f:\n",
    "            l = l.strip('\\n')\n",
    "            tokens = tokenize(l)\n",
    "            X.append(tokens[2:])\n",
    "            Y.append(tokens[0])\n",
    "\n",
    "    #Xe = vec.fit_transform(X)\n",
    "    return X, Y\n",
    "#vec = CountVectorizer(preprocessor = lambda x: x, tokenizer = lambda x: x)    \n",
    "\n",
    "\n",
    "# Create the word clusters\n",
    "create_clusters('/Users/nickeylawrence/Desktop/Chalmers/Academics/ML/Asg2/50mpaths2.txt')\n",
    "\n",
    "# Read the training and test data (X,Y)\n",
    "Xtrain, Ytrain = read_word_clusters('/Users/nickeylawrence/Desktop/Chalmers/Academics/ML/Asg2/train.tsv')\n",
    "Xtest, Ytest = read_word_clusters('/Users/nickeylawrence/Desktop/Chalmers/Academics/ML/Asg2/dev.tsv')\n",
    "\n",
    "#XCluster, YCluster = read_word_clusters('/Users/nickeylawrence/Desktop/Chalmers/Academics/ML/Asg2/train.tsv')\n",
    "#Xtrain, Ytrain = read_feature_descriptions('/Users/nickeylawrence/Desktop/Chalmers/Academics/ML/Asg2/train.tsv')\n",
    "#Xtest, Ytest = read_feature_descriptions('/Users/nickeylawrence/Desktop/Chalmers/Academics/ML/Asg2/dev.tsv')\n",
    "\n",
    "\n",
    "# Preprocessing for tuning hyperparameters using RandomSearch\n",
    "preprocessing_pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    StandardScaler(with_mean=False),\n",
    "    SelectKBest(k=100),\n",
    ")\n",
    "preprocessing_pipeline.fit(Xtrain, Ytrain)\n",
    "X_vec = preprocessing_pipeline.transform(Xtrain)\n",
    "clf = LogisticRegression()\n",
    "C_distr = expon(scale=3)\n",
    "param_grid_random = {'C': C_distr, 'penalty': ['l1', 'l2']}\n",
    "randomsearch = RandomizedSearchCV(clf, param_grid_random, n_iter=10)\n",
    "randomsearch.fit(X_vec, Ytrain);\n",
    "best_C = randomsearch.best_params_['C']\n",
    "best_penalty = randomsearch.best_params_['penalty']\n",
    "\n",
    "\n",
    "# Training and Evaluating the Classifier\n",
    "pipeline = make_pipeline(DictVectorizer(), SelectKBest(k=500), \n",
    "                         LogisticRegression(C=best_C, penalty=best_penalty))\n",
    "\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "cross_validate(pipeline, Xtrain, Ytrain)\n",
    "print(accuracy_score(Ytest, Yguess))\n",
    "\n",
    "#DummyClassifer\n",
    "dummy_pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    DummyClassifier()\n",
    ")\n",
    "dummy_pipeline.fit(Xtrain, Ytrain)\n",
    "Yguess = dummy_pipeline.predict(Xtest)\n",
    "cross_validate(dummy_pipeline, Xtrain, Ytrain)\n",
    "print(accuracy_score(Ytest, Yguess))\n",
    "\n",
    "\n",
    "#Confusion Matrix\n",
    "print(confusion_matrix(Ytest, Yguess, labels = [\"positive\", \"negative\", \"neutral\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
